{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def explorationAlgorithm(explorationAlgorithm, param, num_trials, noiseParams):\n",
    "    cumulativeRewards = []\n",
    "    for i in tqdm(range(num_trials)):\n",
    "        # number of time steps\n",
    "        t = 1000\n",
    "        # number of arms, 10 in this instance\n",
    "        k = 10\n",
    "        # real reward distribution across K arms\n",
    "        rewards = np.random.normal(1, 1, k)\n",
    "        # counts for each arm\n",
    "        n = np.zeros(k)\n",
    "        # extract expected rewards by running specified exploration algorithm with the parameters above\n",
    "        # param is the different, specific parameter for each exploration algorithm\n",
    "        # this would be epsilon for epsilon greedy, initial values for optimistic intialization, c for UCB, and temperature for Boltmann\n",
    "        currentRewards = explorationAlgorithm(\n",
    "            param, t, k, rewards, n, noiseParams=noiseParams)\n",
    "        cumulativeRewards.append(currentRewards)\n",
    "\n",
    "    # TO DO: CALCULATE AVERAGE REWARDS ACROSS EACH ITERATION TO PRODUCE EXPECTED REWARDS\n",
    "    expectedRewards = np.mean(cumulativeRewards, axis=0)\n",
    "    assert len(expectedRewards) == t\n",
    "    return expectedRewards\n",
    "\n",
    "\n",
    "def init_qs(k, value=None) -> np.array:\n",
    "    if value is None:\n",
    "        qs = np.zeros(k)\n",
    "    else:\n",
    "        qs = np.ones(k) * value\n",
    "    return qs\n",
    "\n",
    "\n",
    "def argmax_tiebreak(arr: np.array) -> int:\n",
    "    return np.random.choice(np.flatnonzero(arr == arr.max())) # TODO round-off?\n",
    "\n",
    "\n",
    "def genObservedReward(arm, realRewards, noise_params=(0, 1)):\n",
    "    noise = np.random.normal(noise_params[0], noise_params[1])\n",
    "    return realRewards[arm] + noise\n",
    "\n",
    "\n",
    "def plotExplorations(paramList, selectedAlgorithm, num_trials: int, paramLabelName=\"\", num_steps=1000, noiseParams=(0, 1)):\n",
    "    # TO DO: for each parameter in the param list, plot the returns from the exploration Algorithm from each param on the same plot\n",
    "    x = np.arange(1, 1001)\n",
    "    # calculate your Ys (expected rewards) per each parameter value\n",
    "    # plot all the Ys on the same plot\n",
    "    # include correct labels on your plot!\n",
    "\n",
    "    for param in paramList:\n",
    "        expectedRewards = explorationAlgorithm(\n",
    "            selectedAlgorithm, param=param, num_trials=num_trials, noiseParams=noiseParams)\n",
    "        plt.plot(x, expectedRewards, label=\"=\".join(\n",
    "            [paramLabelName, str(param)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPSILON GREEDY TEMPLATE\n",
    "def epsilonGreedy(epsilon, steps, k, realRewards, n, noiseParams):\n",
    "    # TO DO: initialize structure to hold expected rewards per step\n",
    "    expectedRewards = []\n",
    "    # TO DO: initialize an initial q value for each arm\n",
    "    qs = init_qs(k)\n",
    "    # TO DO: implement the epsilon-greedy algorithm over all steps and return the expected rewards across all steps\n",
    "    for step in range(steps):\n",
    "        # TO DO: choose an arm based on epsilon greedy\n",
    "        if random.random() < epsilon:\n",
    "            arm = random.randint(0, k-1) # random lib: [a, b]\n",
    "        else:\n",
    "            arm = argmax_tiebreak(qs)\n",
    "\n",
    "        observedReward = genObservedReward(arm, realRewards, noiseParams)\n",
    "        n[arm] += 1\n",
    "        qs[arm] = qs[arm] + (observedReward - qs[arm]) / n[arm]\n",
    "        expectedRewards.append((1-epsilon) * np.max(qs) + np.sum(epsilon / k * qs))\n",
    "    return expectedRewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRIALS = 100\n",
    "allEps = [0, 0.001, 0.01, 0.1, 1.0]\n",
    "# allEps = [0, 0.001]\n",
    "plotExplorations(allEps, epsilonGreedy, 100,\n",
    "                 paramLabelName=r\"$\\varepsilon$\", num_steps=1000, noiseParams=(0, 0))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With noise from N(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allEps = [0, 0.001, 0.01, 0.1, 1.0]\n",
    "# allEps = [0, 0.001]\n",
    "plotExplorations(allEps, epsilonGreedy, num_trials=100,\n",
    "                 paramLabelName=r\"$\\varepsilon$\", num_steps=1000, noiseParams=(0, 1))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimistic initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMISTIC INTIALIZATION TEMPLATE\n",
    "def optimisticInitialization(value, steps, k, realRewards, n, noiseParams):\n",
    "    # TO DO: initialize structure to hold expected rewards per step\n",
    "    expectedRewards = []\n",
    "    # TO DO: initialize optimistic initial q values per arm specified by parameter\n",
    "    qs = init_qs(k, value=value)\n",
    "    # TO DO: implement the optimistic initializaiton algorithm over all steps and return the expected rewards across all steps\n",
    "    for step in range(steps):\n",
    "        # TO DO: choose an arm based on epsilon greedy\n",
    "        arm = np.argmax(qs)\n",
    "        observedReward = genObservedReward(arm, realRewards, noiseParams)\n",
    "        expectedRewards.append(realRewards[arm])\n",
    "\n",
    "        n[arm] += 1\n",
    "        qs[arm] = qs[arm] + (observedReward - qs[arm]) / n[arm]\n",
    "\n",
    "    return expectedRewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initValues = [0, 1, 2, 5, 10]\n",
    "# allEps = [0, 0.001]\n",
    "plotExplorations(initValues, optimisticInitialization, num_trials=100,\n",
    "                 paramLabelName=r\"$initVal$\", num_steps=1000, noiseParams=(0, 0))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With noises sampled from N(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initValues = [0, 1, 2, 5, 10]\n",
    "# allEps = [0, 0.001]\n",
    "plotExplorations(initValues, optimisticInitialization, num_trials=100,\n",
    "                 paramLabelName=r\"$initVal$\", num_steps=1000, noiseParams=(0, 1))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# UCB EXPLORATION TEMPLATE\n",
    "def ucbExploration(c, steps, k, realRewards, n: np.array, noiseParams):\n",
    "    # TO DO: initialize structure to hold expected rewards per step\n",
    "    expectedRewards = []\n",
    "    # TO DO: initialize q values per arm\n",
    "    qs = init_qs(k)\n",
    "\n",
    "    # TO DO: implement the UCB exploration algorithm over all steps and return the expected rewards across all steps\n",
    "    for step in range(0, steps):\n",
    "        # TO DO: choose an arm based on epsilon greedy\n",
    "        scores = qs + c * np.sqrt(np.log(step + 1) / (n + 1e-7))\n",
    "        arm = np.argmax(scores)\n",
    "        observedReward = genObservedReward(arm, realRewards, noiseParams)\n",
    "        expectedRewards.append(realRewards[arm])\n",
    "\n",
    "        n[arm] += 1\n",
    "        qs[arm] = qs[arm] + (observedReward - qs[arm]) / n[arm]\n",
    "\n",
    "    return expectedRewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check analytic reward solution with notebook example  \n",
    "\n",
    "![Sutton&Barto text book result](sutton-barto-ucb-fig.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = [0, 1, 2, 5, 10]\n",
    "# cs = [2]\n",
    "# allEps = [0, 0.001]\n",
    "plotExplorations(cs, ucbExploration, num_trials=100,\n",
    "                 paramLabelName=r\"$c$\", num_steps=1000, noiseParams=(0, 0))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### noise from N(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = [0, 1, 2, 5]\n",
    "# allEps = [0, 0.001]\n",
    "plotExplorations(cs, ucbExploration, num_trials=100,\n",
    "                 paramLabelName=r\"$c$\", num_steps=1000, noiseParams=(0, 1))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boltzman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOLTZMANN EXPLORATION TEMPLATE\n",
    "def boltzmannE(temperature, steps, k, realRewards, n, noiseParams):\n",
    "    expectedRewards = []\n",
    "    qs = init_qs(k)\n",
    "\n",
    "    for step in range(steps):\n",
    "        # state copnfiguration for each state (arm)\n",
    "        configs = np.exp(temperature * qs)\n",
    "        config_probs = configs / (np.sum(configs))\n",
    "        # arm = np.argmax(config_probs)\n",
    "        arm = np.random.choice(k, p=config_probs)\n",
    "        observedReward = genObservedReward(arm, realRewards, noiseParams)\n",
    "        expectedRewards.append(np.sum(config_probs * realRewards))\n",
    "        n[arm] += 1\n",
    "        qs[arm] = qs[arm] + (observedReward - qs[arm]) / n[arm]\n",
    "\n",
    "    return expectedRewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed rewards without noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [1, 3, 10, 30, 100]\n",
    "# allEps = [0, 0.001]\n",
    "plotExplorations(temperatures, boltzmannE, num_trials=100,\n",
    "                 paramLabelName=r\"$temperature$\", num_steps=1000, noiseParams=(0, 0))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Withnoises from N(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [1, 3, 10, 30, 100]\n",
    "# allEps = [0, 0.001]\n",
    "plotExplorations(temperatures, boltzmannE, num_trials=100,\n",
    "                 paramLabelName=r\"$temperature$\", num_steps=1000, noiseParams=(0, 1))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.5 Compare the three exploration strategies by plotting the best-performing hyperparameter setting for each method. That is, create a single plot showing expected reward over 1000 time steps with three lines, corresponding to the best hyperparame- ters for ε-greedy, optimistic initialization, and UCB exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithms specific params\n",
    "eps = 0.1\n",
    "initVal = 5\n",
    "ucb_c = 1\n",
    "temperature = 3\n",
    "\n",
    "# trials configs\n",
    "noiseParams = (0, 1)\n",
    "num_trials = 1000\n",
    "\n",
    "epsExpectedRewards = explorationAlgorithm(\n",
    "    epsilonGreedy, param=eps, num_trials=num_trials, noiseParams=noiseParams)\n",
    "\n",
    "optimExpectedRewards = explorationAlgorithm(\n",
    "    optimisticInitialization, param=initVal, num_trials=num_trials, noiseParams=noiseParams)\n",
    "\n",
    "ucbExpectedRewards = explorationAlgorithm(\n",
    "    ucbExploration, param=ucb_c, num_trials=num_trials, noiseParams=noiseParams)\n",
    "boltzmannExpectedRewards = explorationAlgorithm(\n",
    "    boltzmannE, param=temperature, num_trials=num_trials, noiseParams=noiseParams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "xs = np.arange(1, 1000 + 1)\n",
    "sns.lineplot(y=epsExpectedRewards, x=xs, label=r\"$\\varepsilon$=\" + str(eps))\n",
    "sns.lineplot(y=optimExpectedRewards, x=xs, label=\"optim=\" + str(initVal))\n",
    "sns.lineplot(y=ucbExpectedRewards, x=xs, label=\"ucb=\"+str(ucb_c))\n",
    "sns.lineplot(y=boltzmannExpectedRewards, x=xs, label=\"boltzmann=\"+str(temperature))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.6  In 2-3 sentences, explain a setting where you might not want to use the best- performing exploration strategy you found above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('10703-hw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e27e90a569968fbfb09f307bda32fbba698919c30a17b3c05c908f46ea75ae23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
